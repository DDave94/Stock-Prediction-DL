{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TESLA-Sentiment-Analysis-Using-LSTM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyO2gSnx0q1tYVZoHAx2QfQV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_-LiY-dzK8K-"},"source":["# Twitter Sentiment Extraction for Tesla Tweets from 2017 to 2020\n","\n","- Creating text classification model using LSTM deep learning networks. The model was trained using the Sentiment-140 dataset created by Alec Go, Richa Bhayani, and Lei Huang from Standford University\n","\n","- The dataset contains 1.6m tweets labelled as 0-negative or 4-positive. The LSTM model learns to predict the sentiment (positive/negative) of a given tweet and provides a confidence score of each category. \n","\n","- Using trained model to generate sentiment scores for Tesla Tweets\n","\n","    - Cleaning Date Column. Creating Date column with YYYY-MM-DD format\n","\n","    - Cleaning Tweet Content. Removing special characters, mentions, hastags, links, and other special characters from tweets\n","\n","    - Sentiment Extraction. Using the LSTM model to generate the sentiment score for each Tesla tweet\n","\n","    - Aggregating sentiment scores for each day (mean average) to obtain overall sentiment for each day\n","    \n","    - Creating final Twitter dataframe with data from all years of tweets and their daily sentiment scores"]},{"cell_type":"code","metadata":{"id":"501zJQ_1HOyQ","executionInfo":{"status":"ok","timestamp":1617507086696,"user_tz":240,"elapsed":2931,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["# Importing required libraries\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, BatchNormalization, LSTM, Bidirectional, Embedding, Dropout\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","from datetime import datetime\n","\n","pd.set_option(\"display.max_colwidth\" , 100, \"display.max_columns\", 20)\n","np.random.seed(50)\n","tf.random.set_seed(50)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7IKoxGmLEEn"},"source":["## Loading and Cleansing Raw Data\n","- The Sentiment 140 Dataset contains 1.6m tweets classified as positive or negative. Extracting only the tweet and the label from the dataset. Cleaning each tweet before performing vectorization of text.  \n","- Using tweets that are between 100 and 300 in length for training LSTM text classifier\n","\n"," "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q23ZNh8X-ymq","executionInfo":{"status":"ok","timestamp":1617507096405,"user_tz":240,"elapsed":12629,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"82bdd8e3-a0e6-4179-9729-28beb76508cb"},"source":["#Loading Sentiment 140 raw data\n","!wget https://www.dropbox.com/s/ht3rge16u835h24/Sent140.zip?dl=0\n","!unzip -o \"/content/Sent140.zip?dl=0\" "],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-04-04 03:31:26--  https://www.dropbox.com/s/ht3rge16u835h24/Sent140.zip?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6022:18::a27d:4212\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/ht3rge16u835h24/Sent140.zip [following]\n","--2021-04-04 03:31:27--  https://www.dropbox.com/s/raw/ht3rge16u835h24/Sent140.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com/cd/0/inline/BL-KX98OimTEynkw7W1HwNgHAM_1Wi0fouR6vQov2SOTEt76zm6LL_UhaE5_TupQCMSy073tLZWapkfOFfyJlmGVLPfM2ZgliLCz2GQKLEvYSYCTsHnei42ssgynHHk5s67g7tZmtZ2QX9d3rDZpdkkI/file# [following]\n","--2021-04-04 03:31:27--  https://ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com/cd/0/inline/BL-KX98OimTEynkw7W1HwNgHAM_1Wi0fouR6vQov2SOTEt76zm6LL_UhaE5_TupQCMSy073tLZWapkfOFfyJlmGVLPfM2ZgliLCz2GQKLEvYSYCTsHnei42ssgynHHk5s67g7tZmtZ2QX9d3rDZpdkkI/file\n","Resolving ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com (ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6027:15::a27d:480f\n","Connecting to ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com (ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/BL-yRGOEav8tLF0YnUio-i5JSve-rzp4LE6aiEBFKilzX4TVMOgqgQZ19fc3T_blIARTi8gaaiKApfIXX5jJyIuoIclXpgb2NtRde9W4lejqNkKrONhYQwphB9TL6rsV4OAureOr-3y2beKGZoildwp1vBuQX-itWnYnVgTq8uM28Se92yPSmiiTqhaX6fkxCLGfOtVAfnc_MBtOku5VSzUoh-UX_X7pzYSFf-EGMe-EOLbZa4Fvw8r19JuJHFZQQI6JGnOsNnulI0Ve3etZVGmZXouPWtMoXiypBPfa3X237rJpiD6JnxjcyPlYDdv8pcYQdppjEUlFL-jqHgBAMvZtwyQ1x5aK2rtOBw0EgOR4MqUcV8XvvDukDkr-ZjJUTxE/file [following]\n","--2021-04-04 03:31:28--  https://ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com/cd/0/inline2/BL-yRGOEav8tLF0YnUio-i5JSve-rzp4LE6aiEBFKilzX4TVMOgqgQZ19fc3T_blIARTi8gaaiKApfIXX5jJyIuoIclXpgb2NtRde9W4lejqNkKrONhYQwphB9TL6rsV4OAureOr-3y2beKGZoildwp1vBuQX-itWnYnVgTq8uM28Se92yPSmiiTqhaX6fkxCLGfOtVAfnc_MBtOku5VSzUoh-UX_X7pzYSFf-EGMe-EOLbZa4Fvw8r19JuJHFZQQI6JGnOsNnulI0Ve3etZVGmZXouPWtMoXiypBPfa3X237rJpiD6JnxjcyPlYDdv8pcYQdppjEUlFL-jqHgBAMvZtwyQ1x5aK2rtOBw0EgOR4MqUcV8XvvDukDkr-ZjJUTxE/file\n","Reusing existing connection to ucc89da4804f92a1fa7e3bf5777d.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 85745998 (82M) [application/zip]\n","Saving to: ‘Sent140.zip?dl=0’\n","\n","Sent140.zip?dl=0    100%[===================>]  81.77M  17.0MB/s    in 5.4s    \n","\n","2021-04-04 03:31:34 (15.0 MB/s) - ‘Sent140.zip?dl=0’ saved [85745998/85745998]\n","\n","Archive:  /content/Sent140.zip?dl=0\n","  inflating: Sent140.csv             \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQvLrhmPHO0n","executionInfo":{"status":"ok","timestamp":1617507101233,"user_tz":240,"elapsed":17452,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"e203bf89-40b1-4ebb-b8d5-626e12bc9e1c"},"source":["# The Sentiment-140 dataset contains around 1.6 million tweets with labelled sentiments \n","raw_data = pd.read_csv(\"/content/Sent140.csv\", encoding='latin-1')\n","raw_data.columns = ['Label', 'ID', 'Date', 'Flag', 'User', 'Tweet']\n","\n","# Extracting columns required for training\n","raw_data = raw_data[[\"Label\", \"Tweet\"]]\n","\n","#Creating labels for encoding\n","raw_data['Label'] = raw_data['Label'].replace(4,'Positive')\n","raw_data['Label'] = raw_data['Label'].replace(0,'Negative')\n","\n","# Removing tweets that are too small, only keeping tweets between 100 and 300\n","raw_data = raw_data[(raw_data['Tweet'].map(len) > 100) & (raw_data['Tweet'].map(len) < 300)]\n","\n","# Extracting 100000 positive and 100000 negative tweets\n","set_neg = raw_data.iloc[:100000,:]\n","set_pos = raw_data.iloc[-100000:,:]\n","raw_data = pd.concat([set_neg,set_pos],axis = 0)\n","\n","print(raw_data['Label'].value_counts())\n","print(raw_data.shape)\n","print(raw_data.head())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Negative    100000\n","Positive    100000\n","Name: Label, dtype: int64\n","(200000, 2)\n","       Label  \\\n","0   Negative   \n","3   Negative   \n","13  Negative   \n","14  Negative   \n","20  Negative   \n","\n","                                                                                                  Tweet  \n","0   is upset that he can't update his Facebook by texting it... and might cry as a result  School to...  \n","3   @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you a...  \n","13  @smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder'...  \n","14  @iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the pre...  \n","20  one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *s...  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofi-bd4hHO8O","executionInfo":{"status":"ok","timestamp":1617507103616,"user_tz":240,"elapsed":19829,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"e718a504-f010-486f-9811-2ed188289736"},"source":["#Cleaning our labelled training data\n","def tweet_cleanse(text):\n","\n","    #Removing hyperlinks with text\n","    text = re.sub(r'https?:\\/\\/\\S+','', text) \n","\n","    #Removing $ and any text appearing after\n","    text = re.sub(r'\\$[A-za-z0-9]+','', text) \n","\n","    #Removing pattern \"Read More: and MSFT tokens\"\n","    text = re.sub(r'Read more:|MSFT','', text) \n","\n","    #Removing @mentions\n","    text = re.sub(r'@[A-Za-z0-9]+', '', text) \n","\n","    #Removing = sybmol and text coming after\n","    text = re.sub(r'=[\\S\\D\\s]+', '', text)\n","    \n","    #Removing all other special characters\n","    text = re.sub('[^A-Za-z0-9?!\\']+', ' ', text)\n","\n","    return text\n","\n","def gen_clean_tweets(input_df, col): \n","    input_df[col] = input_df[col].apply(lambda x: tweet_cleanse(x))\n","    return input_df\n","\n","#Generating clean tweets from current text\n","raw_data = gen_clean_tweets(raw_data, 'Tweet')\n","print(raw_data.shape)\n","print(raw_data.head())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(200000, 2)\n","       Label  \\\n","0   Negative   \n","3   Negative   \n","13  Negative   \n","14  Negative   \n","20  Negative   \n","\n","                                                                                                  Tweet  \n","0   is upset that he can't update his Facebook by texting it and might cry as a result School today ...  \n","3            no it's not behaving at all i'm mad why am i here? because I can't see you all over there   \n","13   i would've been the first but i didn't have a gun not really though zac snyder's just a douchec...  \n","14                            I wish I got to watch it with you!! I miss you and how was the premiere?!  \n","20     one of my friend called me and asked to meet with her at Mid Valley today but i've no time sigh   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5HhjyMZUl00","executionInfo":{"status":"ok","timestamp":1617507103617,"user_tz":240,"elapsed":19825,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"3b94423a-183d-460c-f65b-1e1c8eeeea6e"},"source":["#Extracting Raw Tweets and Labels \n","def extract_tweet_labels(clean_data):\n","    raw_tweet, raw_y = clean_data[\"Tweet\"].values, clean_data[\"Label\"].values\n","    return raw_tweet, raw_y\n","\n","# Shufffling the raw data to disorient the negative and positive sentiments\n","# So that when we create training and testing data, we get instances of both labels\n","raw_data = raw_data.sample(frac=1, random_state=50)\n","\n","# Creating Raw X (tweets) and Raw Y (classified label)\n","raw_X, raw_y = extract_tweet_labels(raw_data)\n","print(raw_X)\n","print(raw_y)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["['just got back from quot work quot and is already tired still going to lunch with madee! I need my friends '\n"," 'okay my dear tweets! i have to get up early and hang out with the small chitlins again tomorrow! love yous guys! night!'\n"," 'haha six flags was amazing wanted to go on king da ka but it was closed other than that awesome two thumbs up!!!!! '\n"," ...\n"," ' I saw the previews for land of the lost and thought Ummmm NO! LOL Thanks for confirming my thought on that '\n"," ' Exactly Numbers on twitter mean nothing unless you actually quot connect quot with your followers And you do! Smart man '\n"," 'Have a listen to our music on Mixposure a great place to see some fantastic reviews ']\n","['Positive' 'Positive' 'Negative' ... 'Positive' 'Positive' 'Positive']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J1dsBhwWTPrW"},"source":["## Creating Training Data and Testing Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naC7iFxmSPFf","executionInfo":{"status":"ok","timestamp":1617507112539,"user_tz":240,"elapsed":28742,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"9dde2ae3-b098-4228-e4ad-f759c3b2f0e6"},"source":["# Vectorizing the tweets\n","# Limit the dataset to top 50000 words \n","max_words = 50000\n","\n","# Finding Max number of words in each tweet\n","max_tweet_len = 300\n","\n","#Embedding size for vecotr embedding layer\n","embedding_size = 128\n","\n","#Creating a tokenizer object and vectorizing the tweets  \n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(raw_X)\n","X_tokenized = tokenizer.texts_to_sequences(raw_X)\n","\n","#Truncating the input sequences to the max tweet length\n","X = pad_sequences(X_tokenized, padding='post', maxlen=max_tweet_len)\n","print('Shape of X:', X.shape)\n","print(raw_X[0])\n","print(X[0])\n","print(raw_y[0])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Shape of X: (200000, 300)\n","just got back from quot work quot and is already tired still going to lunch with madee! I need my friends \n","[   20    46    60    43    26    62    26     5    12   219   259    77\n","    52     3   441    23 47612     1    92     6   197     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0]\n","Positive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBDJw7LkkIBA","executionInfo":{"status":"ok","timestamp":1617507112539,"user_tz":240,"elapsed":28736,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"d42c1d8a-2d80-4f1a-a461-775b031f6321"},"source":["# Encoding Y to create labels\n","Y = pd.get_dummies(raw_y).values\n","print('Shape Encoded Y: ', Y.shape)\n","print('\\nFirst 5 labels of Y:\\n', Y[0:5])\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Shape Encoded Y:  (200000, 2)\n","\n","First 5 labels of Y:\n"," [[0 1]\n"," [0 1]\n"," [1 0]\n"," [0 1]\n"," [0 1]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRojtgMfquCz","executionInfo":{"status":"ok","timestamp":1617507112539,"user_tz":240,"elapsed":28731,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"0b8044c6-3809-47bf-809e-db8912e42909"},"source":["#Train Test Split\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42, shuffle=True)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(180000, 300) (180000, 2)\n","(20000, 300) (20000, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MpruuOxnZnGj"},"source":["## Sentiment Classficiation Model Training and Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fkZH4xEFZpmD","executionInfo":{"status":"ok","timestamp":1617507652136,"user_tz":240,"elapsed":568323,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"bcacda84-6689-4b4f-adb9-712533cb60cc"},"source":["# Model Definition\n","def build_model():\n","    \n","    model = Sequential()\n","    model.add(Embedding(max_words, embedding_size,input_length=X.shape[1]))\n","\n","    model.add(Bidirectional(LSTM(units = 128)))\n","    model.add(Dropout(0.5))\n","\n","    model.add(Dense(units = 16, activation='relu'))\n","    model.add(Dropout(0.5))\n","\n","    model.add(Dense(units = 2, activation='softmax'))\n","    print(model.summary())\n","\n","    return model \n","\n","# Compiling Model\n","def compile_model(model):\n","    \n","    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics = ['accuracy'])\n","    return model\n","\n","# Training Model\n","def train_model(model, X_tr, Y_tr):\n","    batch_size = 256\n","    epochs = 8\n","    history = model.fit(X_tr, Y_tr, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.1)\n","    return model, history\n","\n","# Model Evaluation\n","def eval_model(m, test_X, test_Y):\n","\n","    test_loss, test_accuracy = m.evaluate(test_X, test_Y, verbose=0)\n","            \n","    print('MODEL EVALUATION : \\n')\n","    print(\"The testing set loss : \", test_loss)\n","    print(\"The testing set accuracy : \", test_accuracy)\n","       \n","    return None\n","\n","\n","model = build_model()\n","model = compile_model(model)\n","model, history = train_model(model, X_train, Y_train)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 300, 128)          6400000   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 256)               263168    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 256)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 16)                4112      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 16)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 34        \n","=================================================================\n","Total params: 6,667,314\n","Trainable params: 6,667,314\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/8\n","633/633 [==============================] - 83s 104ms/step - loss: 0.6562 - accuracy: 0.5854 - val_loss: 0.5134 - val_accuracy: 0.7505\n","Epoch 2/8\n","633/633 [==============================] - 65s 102ms/step - loss: 0.5045 - accuracy: 0.7696 - val_loss: 0.4992 - val_accuracy: 0.7588\n","Epoch 3/8\n","633/633 [==============================] - 65s 102ms/step - loss: 0.4698 - accuracy: 0.7947 - val_loss: 0.4984 - val_accuracy: 0.7598\n","Epoch 4/8\n","633/633 [==============================] - 65s 102ms/step - loss: 0.4467 - accuracy: 0.8102 - val_loss: 0.5071 - val_accuracy: 0.7581\n","Epoch 5/8\n","633/633 [==============================] - 64s 102ms/step - loss: 0.4245 - accuracy: 0.8240 - val_loss: 0.5170 - val_accuracy: 0.7530\n","Epoch 6/8\n","633/633 [==============================] - 65s 102ms/step - loss: 0.4077 - accuracy: 0.8331 - val_loss: 0.5324 - val_accuracy: 0.7480\n","Epoch 7/8\n","633/633 [==============================] - 65s 102ms/step - loss: 0.3976 - accuracy: 0.8389 - val_loss: 0.5427 - val_accuracy: 0.7448\n","Epoch 8/8\n","633/633 [==============================] - 65s 102ms/step - loss: 0.3854 - accuracy: 0.8448 - val_loss: 0.5775 - val_accuracy: 0.7408\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vscTvnsB8VLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617507660023,"user_tz":240,"elapsed":576205,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"cc2e7201-953b-494b-9b8d-8d57e3932067"},"source":["# Printing Confusion Matrix, Classification Report\n","tf.keras.utils.plot_model(model, to_file='Sentiment Analysis Model.png', show_shapes=True)\n","eval_model(model, X_test, Y_test)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["MODEL EVALUATION : \n","\n","The testing set loss :  0.5739759802818298\n","The testing set accuracy :  0.7409499883651733\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJimnWi8ETFa","executionInfo":{"status":"ok","timestamp":1617507660749,"user_tz":240,"elapsed":576925,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"d912eb32-fffd-4246-d84e-b0504eb5f6a0"},"source":["#Using the Model to generate sentiment confidence scores for sample tweet\n","tweets = [[\"I pretty sure Tesla's stock price is going to double by this time next yr\"]]\n","\n","for sample_tweet in tweets:\n","    seq = tokenizer.texts_to_sequences(sample_tweet)\n","    padded = pad_sequences(seq, padding='post', maxlen=300)\n","\n","    pred = model.predict(padded)\n","    print(pred)\n","\n","    if (np.argmax(pred[0]) == 0):\n","        sentiment_score = -1 * pred[0][0]\n","    elif (np.argmax(pred[0]) == 1):\n","        sentiment_score = pred[0][1] \n","\n","    print (sentiment_score)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[0.6517518  0.34824818]]\n","-0.6517518162727356\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Muyf5KlrN5_i"},"source":["# Creating Sentiment Scores for Tesla Tweets\n","- Using the trained model from above to classify tweets related to Tesla"]},{"cell_type":"code","metadata":{"id":"J3zgftg9OVru","executionInfo":{"status":"ok","timestamp":1617507662141,"user_tz":240,"elapsed":578316,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["#Loading raw twitter data containing Date and Tweets pertaining to Tesla Stock\n","df_2017 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Tesla/tsla-tweets-2017.csv\")\n","df_2018 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Tesla/tsla-tweets-2018.csv\")\n","df_2019 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Tesla/tsla-tweets-2019.csv\")\n","df_2020 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Tesla/tsla-tweets-2020.csv\")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdItrWjMo600"},"source":["## Cleaning Tesla Tweets - Date Column\n","###### *Creating Date column by removing time component, since we want to aggregrate sentiment scores on a daily basis*"]},{"cell_type":"code","metadata":{"id":"fpJ8-7h5ns5y","executionInfo":{"status":"ok","timestamp":1617507662142,"user_tz":240,"elapsed":578315,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["def date_generation(date):\n","    # Create date object from given time format in dataframe\n","    my_date = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S+00:00\")\n","    return my_date.date()\n","\n","def date_cleanse(input_df):\n","    input_df['Date'] = input_df['Datetime'].apply(lambda x: date_generation(x))\n","    return input_df\n","\n","# Creating clean date columns\n","df_2017 = date_cleanse(df_2017)\n","df_2018 = date_cleanse(df_2018)\n","df_2019 = date_cleanse(df_2019)\n","df_2020 = date_cleanse(df_2020)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRW6JTK9p8KN"},"source":["## Cleaning Tweet content\n","###### *Removing links, mentions, hashtags etc.*"]},{"cell_type":"code","metadata":{"id":"WlFG1wqfucuv","executionInfo":{"status":"ok","timestamp":1617507662407,"user_tz":240,"elapsed":578578,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["#Generating clean tweets from current text\n","df_2017 = gen_clean_tweets(df_2017, 'Text')\n","df_2018 = gen_clean_tweets(df_2018, 'Text')\n","df_2019 = gen_clean_tweets(df_2019, 'Text')\n","df_2020 = gen_clean_tweets(df_2020, 'Text')"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZyvnUwJTdNO"},"source":["## Tweet Sentiment Calculations\n","###### *Generating the sentiment score for each tweet using the trained LSTM text classifier model*"]},{"cell_type":"code","metadata":{"id":"tr7LImZDfBU1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617508230368,"user_tz":240,"elapsed":1146533,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"5fc68b20-ba70-4c38-df9d-40f4354d1d92"},"source":["def get_sentiment(tweet, sent_model): \n","    \n","    #Vectorizing and Padding tweet using max length of Tesla tweets\n","    tweet_tokenized = tokenizer.texts_to_sequences([tweet])\n","    tweet_padded = pad_sequences(tweet_tokenized, padding='post', maxlen=300)\n","\n","    #Generating prediction using trained model\n","    pred = sent_model.predict(tweet_padded)\n","    \n","    #Extracting the sentiment confidence score\n","    if (np.argmax(pred[0]) == 0):\n","        sentiment_score = -1 * pred[0][0]\n","    elif (np.argmax(pred[0]) == 1):\n","        sentiment_score = pred[0][1] \n","\n","    return sentiment_score\n","\n","#Creates a sentiment score for each tweet using model and Tesla tweets\n","def sentiment_generation(sent_model, input_df): \n","\n","    max_len = input_df['Text'].apply(len).max()\n","    input_df['TwitterSentiment'] = input_df['Text'].apply(get_sentiment, sent_model = sent_model)\n","    sentiment_df = input_df\n","\n","    return sentiment_df \n","\n","df_2017 = sentiment_generation(model, df_2017)\n","df_2018 = sentiment_generation(model, df_2018)\n","df_2019 = sentiment_generation(model, df_2019)\n","df_2020 = sentiment_generation(model, df_2020)\n","\n","print(df_2017[['Text', 'TwitterSentiment']].head(10))\n","df_2017.to_csv(\"/content/TweetsAndScores.csv\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["                                                                                                  Text  \\\n","0                                             Tesla's Stock Starts 2017 At A Critical Juncture Forbes    \n","1                                             Tesla s Stock Starts 2017 At A Critical Juncture Forbes    \n","2                             Tesla's Stock Starts 2017 At A Critical Juncture Forbes Google News Tech   \n","3                                     Tesla's Stock Starts 2017 At A Critical Juncture Forbes ggtechmy   \n","4                                    ggtechmy Tesla's Stock Starts 2017 At A Critical Juncture Forbes    \n","5                                             Tesla's Stock Starts 2017 At A Critical Juncture Forbes    \n","6  Tesla's Stock Starts 2017 At A Critical Juncture Forbes technology Tesla's Stock Starts 2017 At ...   \n","7                                   Tesla's Stock Starts 2017 At A Critical Juncture Forbes technology   \n","8                                   Tesla's Stock Starts 2017 At A Critical Juncture Forbes technology   \n","9                            Tesla's Stock Starts 2017 At A Critical Juncture Forbes Startup tech news   \n","\n","   TwitterSentiment  \n","0         -0.797110  \n","1         -0.754465  \n","2         -0.721515  \n","3         -0.797110  \n","4         -0.797110  \n","5         -0.797110  \n","6         -0.896365  \n","7         -0.785368  \n","8         -0.785368  \n","9         -0.758690  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-BIlsbK7qAbt"},"source":["##Sentiment Score Aggregation\n","###### *Aggregating the final sentiment scores to gather scores for each day*"]},{"cell_type":"code","metadata":{"id":"CMsrew4CqJst","executionInfo":{"status":"ok","timestamp":1617508230369,"user_tz":240,"elapsed":1146533,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["#Creates an aggregated sentiment score for each day of tweets\n","def aggregated_df (input_df):\n","    agg_df = input_df[['Date', 'TwitterSentiment']]\n","    agg_df =  input_df.groupby([\"Date\"], as_index=False)['TwitterSentiment'].mean()\n","    return agg_df\n","\n","final_2017 = aggregated_df(df_2017)\n","final_2018 = aggregated_df(df_2018)\n","final_2019 = aggregated_df(df_2019)\n","final_2020 = aggregated_df(df_2020)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKzZLKb23bot"},"source":["##Creating full input dataframe for further analysis and stock price predictions\n","###### *Combining twitter data from 2017,2018,2019,2020*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAONYIidb4w2","executionInfo":{"status":"ok","timestamp":1617508230370,"user_tz":240,"elapsed":1146529,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"59fe8546-c384-43cb-b26b-ae8db5eb48ea"},"source":["#Creating full twitter sentiment data\n","twitter_data = final_2017.append([final_2018, final_2019, final_2020])\n","print(twitter_data.shape)\n","print(twitter_data.head())\n","print(twitter_data.tail())\n","\n","twitter_data.to_csv('/content/tsla-tweet-sentiments-lstm.csv', index=False, encoding= 'utf-8-sig') "],"execution_count":17,"outputs":[{"output_type":"stream","text":["(1456, 2)\n","         Date  TwitterSentiment\n","0  2017-01-01         -0.789756\n","1  2017-01-02          0.408966\n","2  2017-01-03          0.282842\n","3  2017-01-04         -0.129541\n","4  2017-01-05         -0.351872\n","           Date  TwitterSentiment\n","359  2020-12-25          0.014247\n","360  2020-12-26          0.068771\n","361  2020-12-27          0.088822\n","362  2020-12-28         -0.170788\n","363  2020-12-29          0.525323\n"],"name":"stdout"}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MSFT-Sentiment-Analysis-Using-LSTM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyM+/Yj19tCjGYhsAkA5sDyc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_-LiY-dzK8K-"},"source":["# Twitter Sentiment Extraction for Microsoft Tweets from 2017 to 2020\n","\n","- Creating text classification model using LSTM deep learning networks. The model was trained using the Sentiment-140 dataset created by Alec Go, Richa Bhayani, and Lei Huang from Standford University\n","\n","- The dataset contains 1.6m tweets labelled as 0-negative or 4-positive. The LSTM model learns to predict the sentiment (positive/negative) of a given tweet and provides a confidence score of each category. \n","\n","- Using trained model to generate sentiment scores for Microsoft Tweet \n","\n","    - Cleaning Date Column. Creating Date column with YYYY-MM-DD format\n","\n","    - Cleaning Tweet Content. Removing special characters, mentions, hastags, links, and other special characters from tweets\n","\n","    - Sentiment Extraction. Using the LSTM model to generate the sentiment score for each Microsoft tweet\n","\n","    - Aggregating sentiment scores for each day (mean average) to obtain overall sentiment for each day\n","    \n","    - Creating final Twitter dataframe with data from all years of tweets and their daily sentiment scores"]},{"cell_type":"code","metadata":{"id":"501zJQ_1HOyQ","executionInfo":{"status":"ok","timestamp":1617399957563,"user_tz":240,"elapsed":2214,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["# Importing required libraries\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, BatchNormalization, LSTM, Bidirectional, Embedding, Dropout\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","from datetime import datetime\n","\n","pd.set_option(\"display.max_colwidth\" , 100, \"display.max_columns\", 20)\n","np.random.seed(50)\n","tf.random.set_seed(50)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7IKoxGmLEEn"},"source":["## Loading and Cleansing Raw Data\n","- The Sentiment 140 Dataset contains 1.6m tweets classified as positive or negative. Extracting only the tweet and the label from the dataset. Cleaning each tweet before performing vectorization of text.  \n","- Using tweets that are between 100 and 300 in length for training LSTM text classifier\n","\n"," "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q23ZNh8X-ymq","executionInfo":{"status":"ok","timestamp":1617399968748,"user_tz":240,"elapsed":13392,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"38b59c1f-6ebf-4843-8e74-2b493c31a001"},"source":["#Loading Sentiment 140 raw data\n","!wget https://www.dropbox.com/s/ht3rge16u835h24/Sent140.zip?dl=0\n","!unzip -o \"/content/Sent140.zip?dl=0\" "],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-04-02 21:45:57--  https://www.dropbox.com/s/ht3rge16u835h24/Sent140.zip?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.18, 2620:100:6032:18::a27d:5212\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/ht3rge16u835h24/Sent140.zip [following]\n","--2021-04-02 21:45:57--  https://www.dropbox.com/s/raw/ht3rge16u835h24/Sent140.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com/cd/0/inline/BL7UwSMUUYvkL5wcHL5Sf4GaD6kdHQCCEsOKQogPIXZ3VWs0C0bmvKjhRp881ilQ0nIowg7wHkJi1Qaph06wU139MPHYvoMB5DgDulBd1-7rrnPP-I8Sq0uAPArm6ksNjjYObddlWEOLG6DU5RuztkZQ/file# [following]\n","--2021-04-02 21:45:57--  https://uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com/cd/0/inline/BL7UwSMUUYvkL5wcHL5Sf4GaD6kdHQCCEsOKQogPIXZ3VWs0C0bmvKjhRp881ilQ0nIowg7wHkJi1Qaph06wU139MPHYvoMB5DgDulBd1-7rrnPP-I8Sq0uAPArm6ksNjjYObddlWEOLG6DU5RuztkZQ/file\n","Resolving uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com (uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com)... 162.125.82.15, 2620:100:6032:15::a27d:520f\n","Connecting to uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com (uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/BL7sYk01P8SbOh1QSyxx3fhB2hiNtxTTAMtZ1ldQHAPbvjzpJ8YeAyikS8JLWRkXyoWVdgHfgPKEV3MLUE5sht0kvBDlYrSadLZg29DMPKMwvFEiLjULrwFH5FvJuHSZYSFeEESqTo9ORQuJLpfto9OCtH4c_5ri7Nhl_5OJUkVMd2JEp-Cq8LEYtavazZzJ5zwHwQaMqM1MEYTI5WZk2YpZITWXadh8gjUpCDUxR1qdqzZVnYpCARM3sw93t7HXpGg8j8UUnsaeiS4IHQz1ax0ePIX-EK7rg0nggiX4KauK7WAduXF00y741DO1ZTuJDy9lAfGkJOY_GhSmOrhqE63yA7hrohCNqgCDMYuBPSS9XL5Z73zZswfNAtQixQwMVlU/file [following]\n","--2021-04-02 21:45:59--  https://uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com/cd/0/inline2/BL7sYk01P8SbOh1QSyxx3fhB2hiNtxTTAMtZ1ldQHAPbvjzpJ8YeAyikS8JLWRkXyoWVdgHfgPKEV3MLUE5sht0kvBDlYrSadLZg29DMPKMwvFEiLjULrwFH5FvJuHSZYSFeEESqTo9ORQuJLpfto9OCtH4c_5ri7Nhl_5OJUkVMd2JEp-Cq8LEYtavazZzJ5zwHwQaMqM1MEYTI5WZk2YpZITWXadh8gjUpCDUxR1qdqzZVnYpCARM3sw93t7HXpGg8j8UUnsaeiS4IHQz1ax0ePIX-EK7rg0nggiX4KauK7WAduXF00y741DO1ZTuJDy9lAfGkJOY_GhSmOrhqE63yA7hrohCNqgCDMYuBPSS9XL5Z73zZswfNAtQixQwMVlU/file\n","Reusing existing connection to uc2bdb97605ddf444ca234e46ba1.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 85745998 (82M) [application/zip]\n","Saving to: ‘Sent140.zip?dl=0’\n","\n","Sent140.zip?dl=0    100%[===================>]  81.77M  14.9MB/s    in 6.2s    \n","\n","2021-04-02 21:46:06 (13.1 MB/s) - ‘Sent140.zip?dl=0’ saved [85745998/85745998]\n","\n","Archive:  /content/Sent140.zip?dl=0\n","  inflating: Sent140.csv             \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQvLrhmPHO0n","executionInfo":{"status":"ok","timestamp":1617399973426,"user_tz":240,"elapsed":18065,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"bc4d94d5-faf6-4791-da53-6cc8a5d78d7b"},"source":["# The Sentiment-140 dataset contains around 1.6 million tweets with labelled sentiments \n","raw_data = pd.read_csv(\"/content/Sent140.csv\", encoding='latin-1')\n","raw_data.columns = ['Label', 'ID', 'Date', 'Flag', 'User', 'Tweet']\n","\n","# Extracting columns required for training\n","raw_data = raw_data[[\"Label\", \"Tweet\"]]\n","\n","#Creating labels for encoding\n","raw_data['Label'] = raw_data['Label'].replace(4,'Positive')\n","raw_data['Label'] = raw_data['Label'].replace(0,'Negative')\n","\n","# Removing tweets that are too small, only keeping tweets between 100 and 300\n","raw_data = raw_data[(raw_data['Tweet'].map(len) > 100) & (raw_data['Tweet'].map(len) < 300)]\n","\n","# Extracting 100000 positive and 100000 negative tweets\n","set_neg = raw_data.iloc[:100000,:]\n","set_pos = raw_data.iloc[-100000:,:]\n","raw_data = pd.concat([set_neg,set_pos],axis = 0)\n","\n","print(raw_data['Label'].value_counts())\n","print(raw_data.shape)\n","print(raw_data.head())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Positive    100000\n","Negative    100000\n","Name: Label, dtype: int64\n","(200000, 2)\n","       Label  \\\n","0   Negative   \n","3   Negative   \n","13  Negative   \n","14  Negative   \n","20  Negative   \n","\n","                                                                                                  Tweet  \n","0   is upset that he can't update his Facebook by texting it... and might cry as a result  School to...  \n","3   @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you a...  \n","13  @smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder'...  \n","14  @iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the pre...  \n","20  one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *s...  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofi-bd4hHO8O","executionInfo":{"status":"ok","timestamp":1617399975764,"user_tz":240,"elapsed":20398,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"b092cd34-0ba2-4356-8093-c03d697697ee"},"source":["#Cleaning our labelled training data\n","def tweet_cleanse(text):\n","\n","    #Removing hyperlinks with text\n","    text = re.sub(r'https?:\\/\\/\\S+','', text) \n","\n","    #Removing $ and any text appearing after\n","    text = re.sub(r'\\$[A-za-z0-9]+','', text) \n","\n","    #Removing pattern \"Read More: and MSFT tokens\"\n","    text = re.sub(r'Read more:|MSFT','', text) \n","\n","    #Removing @mentions\n","    text = re.sub(r'@[A-Za-z0-9]+', '', text) \n","\n","    #Removing = sybmol and text coming after\n","    text = re.sub(r'=[\\S\\D\\s]+', '', text)\n","    \n","    #Removing all other special characters\n","    text = re.sub('[^A-Za-z0-9?!\\']+', ' ', text)\n","\n","    return text\n","\n","def gen_clean_tweets(input_df, col): \n","    input_df[col] = input_df[col].apply(lambda x: tweet_cleanse(x))\n","    return input_df\n","\n","#Generating clean tweets from current text\n","raw_data = gen_clean_tweets(raw_data, 'Tweet')\n","print(raw_data.shape)\n","print(raw_data.head())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(200000, 2)\n","       Label  \\\n","0   Negative   \n","3   Negative   \n","13  Negative   \n","14  Negative   \n","20  Negative   \n","\n","                                                                                                  Tweet  \n","0   is upset that he can't update his Facebook by texting it and might cry as a result School today ...  \n","3            no it's not behaving at all i'm mad why am i here? because I can't see you all over there   \n","13   i would've been the first but i didn't have a gun not really though zac snyder's just a douchec...  \n","14                            I wish I got to watch it with you!! I miss you and how was the premiere?!  \n","20     one of my friend called me and asked to meet with her at Mid Valley today but i've no time sigh   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5HhjyMZUl00","executionInfo":{"status":"ok","timestamp":1617399975765,"user_tz":240,"elapsed":20394,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"510e4fb9-a779-4b63-bfd8-d7e01f24f023"},"source":["#Extracting Raw Tweets and Labels \n","def extract_tweet_labels(clean_data):\n","    raw_tweet, raw_y = clean_data[\"Tweet\"].values, clean_data[\"Label\"].values\n","    return raw_tweet, raw_y\n","\n","# Shufffling the raw data to disorient the negative and positive sentiments\n","# So that when we create training and testing data, we get instances of both labels\n","raw_data = raw_data.sample(frac=1, random_state=50)\n","\n","# Creating Raw X (tweets) and Raw Y (classified label)\n","raw_X, raw_y = extract_tweet_labels(raw_data)\n","print(raw_X)\n","print(raw_y)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["['just got back from quot work quot and is already tired still going to lunch with madee! I need my friends '\n"," 'okay my dear tweets! i have to get up early and hang out with the small chitlins again tomorrow! love yous guys! night!'\n"," 'haha six flags was amazing wanted to go on king da ka but it was closed other than that awesome two thumbs up!!!!! '\n"," ...\n"," ' I saw the previews for land of the lost and thought Ummmm NO! LOL Thanks for confirming my thought on that '\n"," ' Exactly Numbers on twitter mean nothing unless you actually quot connect quot with your followers And you do! Smart man '\n"," 'Have a listen to our music on Mixposure a great place to see some fantastic reviews ']\n","['Positive' 'Positive' 'Negative' ... 'Positive' 'Positive' 'Positive']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J1dsBhwWTPrW"},"source":["## Creating Training Data and Testing Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naC7iFxmSPFf","executionInfo":{"status":"ok","timestamp":1617399984442,"user_tz":240,"elapsed":29066,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"8281906a-62cd-42b8-cf57-6b539b7010af"},"source":["# Vectorizing the tweets\n","# Limit the dataset to top 50000 words \n","max_words = 50000\n","\n","# Finding Max number of words in each tweet\n","max_tweet_len = 300\n","\n","#Embedding size for vecotr embedding layer\n","embedding_size = 128\n","\n","#Creating a tokenizer object and vectorizing the tweets  \n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(raw_X)\n","X_tokenized = tokenizer.texts_to_sequences(raw_X)\n","\n","#Truncating the input sequences to the max tweet length\n","X = pad_sequences(X_tokenized, padding='post', maxlen=max_tweet_len)\n","print('Shape of X:', X.shape)\n","print(raw_X[0])\n","print(X[0])\n","print(raw_y[0])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Shape of X: (200000, 300)\n","just got back from quot work quot and is already tired still going to lunch with madee! I need my friends \n","[   20    46    60    43    26    62    26     5    12   219   259    77\n","    52     3   441    23 47612     1    92     6   197     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0]\n","Positive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBDJw7LkkIBA","executionInfo":{"status":"ok","timestamp":1617399984443,"user_tz":240,"elapsed":29062,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"5450ee85-a3d4-4d9e-bff9-dc25f073995e"},"source":["# Encoding Y to create labels\n","Y = pd.get_dummies(raw_y).values\n","print('Shape Encoded Y: ', Y.shape)\n","print('\\nFirst 5 labels of Y:\\n', Y[0:5])\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Shape Encoded Y:  (200000, 2)\n","\n","First 5 labels of Y:\n"," [[0 1]\n"," [0 1]\n"," [1 0]\n"," [0 1]\n"," [0 1]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRojtgMfquCz","executionInfo":{"status":"ok","timestamp":1617399984443,"user_tz":240,"elapsed":29058,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"68fa586c-fb96-4071-dff1-321e207999d8"},"source":["#Train Test Split\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42, shuffle=True)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(180000, 300) (180000, 2)\n","(20000, 300) (20000, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MpruuOxnZnGj"},"source":["## Sentiment Classficiation Model Training and Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fkZH4xEFZpmD","executionInfo":{"status":"ok","timestamp":1617400512936,"user_tz":240,"elapsed":557546,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"6c2b1dc4-adca-4bed-d9f4-ed47ed91b3c0"},"source":["# Model Definition\n","def build_model():\n","    \n","    model = Sequential()\n","    model.add(Embedding(max_words, embedding_size,input_length=X.shape[1]))\n","\n","    model.add(Bidirectional(LSTM(units = 128)))\n","    model.add(Dropout(0.5))\n","\n","    model.add(Dense(units = 16, activation='relu'))\n","    model.add(Dropout(0.5))\n","\n","    model.add(Dense(units = 2, activation='softmax'))\n","    print(model.summary())\n","\n","    return model \n","\n","# Compiling Model\n","def compile_model(model):\n","    \n","    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics = ['accuracy'])\n","    return model\n","\n","# Training Model\n","def train_model(model, X_tr, Y_tr):\n","    batch_size = 256\n","    epochs = 8\n","    history = model.fit(X_tr, Y_tr, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.1)\n","    return model, history\n","\n","# Model Evaluation\n","def eval_model(m, test_X, test_Y):\n","\n","    test_loss, test_accuracy = m.evaluate(test_X, test_Y, verbose=0)\n","            \n","    print('MODEL EVALUATION : \\n')\n","    print(\"The testing set loss : \", test_loss)\n","    print(\"The testing set accuracy : \", test_accuracy)\n","       \n","    return None\n","\n","\n","model = build_model()\n","model = compile_model(model)\n","model, history = train_model(model, X_train, Y_train)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 300, 128)          6400000   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 256)               263168    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 256)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 16)                4112      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 16)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 34        \n","=================================================================\n","Total params: 6,667,314\n","Trainable params: 6,667,314\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/8\n","633/633 [==============================] - 70s 104ms/step - loss: 0.6563 - accuracy: 0.5854 - val_loss: 0.5128 - val_accuracy: 0.7502\n","Epoch 2/8\n","633/633 [==============================] - 65s 103ms/step - loss: 0.5044 - accuracy: 0.7692 - val_loss: 0.4982 - val_accuracy: 0.7597\n","Epoch 3/8\n","633/633 [==============================] - 66s 104ms/step - loss: 0.4696 - accuracy: 0.7949 - val_loss: 0.4977 - val_accuracy: 0.7608\n","Epoch 4/8\n","633/633 [==============================] - 65s 103ms/step - loss: 0.4465 - accuracy: 0.8106 - val_loss: 0.5095 - val_accuracy: 0.7578\n","Epoch 5/8\n","633/633 [==============================] - 65s 103ms/step - loss: 0.4241 - accuracy: 0.8241 - val_loss: 0.5172 - val_accuracy: 0.7533\n","Epoch 6/8\n","633/633 [==============================] - 65s 103ms/step - loss: 0.4076 - accuracy: 0.8327 - val_loss: 0.5362 - val_accuracy: 0.7451\n","Epoch 7/8\n","633/633 [==============================] - 65s 103ms/step - loss: 0.3974 - accuracy: 0.8391 - val_loss: 0.5442 - val_accuracy: 0.7448\n","Epoch 8/8\n","633/633 [==============================] - 65s 103ms/step - loss: 0.3862 - accuracy: 0.8438 - val_loss: 0.5775 - val_accuracy: 0.7396\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vscTvnsB8VLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617400520857,"user_tz":240,"elapsed":565462,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"438a316f-12de-4644-82f3-3902a406005e"},"source":["# Printing Confusion Matrix, Classification Report\n","eval_model(model, X_test, Y_test)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["MODEL EVALUATION : \n","\n","The testing set loss :  0.5745931267738342\n","The testing set accuracy :  0.7399500012397766\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJimnWi8ETFa","executionInfo":{"status":"ok","timestamp":1617400521273,"user_tz":240,"elapsed":565874,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"675d5cce-b514-4b0f-800a-c48a80bdbb3c"},"source":["#Using the Model to generate sentiment confidence scores for sample tweet\n","tweets = [['Christopher C Capossela Sells 4 000 Shares of Microsoft Co Stock'], ['Microsoft Corporation Stock Shares Spike Down As Earnings Are Below MicrosoftCorporation ']]\n","\n","for sample_tweet in tweets:\n","    seq = tokenizer.texts_to_sequences(sample_tweet)\n","    padded = pad_sequences(seq, padding='post', maxlen=300)\n","\n","    pred = model.predict(padded)\n","    print(pred)\n","\n","    if (np.argmax(pred[0]) == 0):\n","        sentiment_score = -1 * pred[0][0]\n","    elif (np.argmax(pred[0]) == 1):\n","        sentiment_score = pred[0][1] \n","\n","    print (sentiment_score)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[0.545996 0.454004]]\n","-0.5459960103034973\n","[[0.8312995  0.16870055]]\n","-0.8312994837760925\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Muyf5KlrN5_i"},"source":["# Creating Sentiment Scores for Microsoft Tweets\n","- Using the trained model from above to classify tweets related to Microsoft"]},{"cell_type":"code","metadata":{"id":"J3zgftg9OVru","executionInfo":{"status":"ok","timestamp":1617400523092,"user_tz":240,"elapsed":567689,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["#Loading raw twitter data containing tweets pertaining to Microsoft Stock\n","df_2017 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Microsoft/msft-tweets-2017.csv\")\n","df_2018 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Microsoft/msft-tweets-2018.csv\")\n","df_2019 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Microsoft/msft-tweets-2019.csv\")\n","df_2020 = pd.read_csv(\"https://raw.githubusercontent.com/DDave94/Stock-Prediction-DL/main/datasets/raw/Microsoft/msft-tweets-2020.csv\")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdItrWjMo600"},"source":["## Cleaning Microsoft Tweets - Date Column\n","###### *Creating Date column by removing time component, since we want to aggregrate sentiment scores on a daily basis*"]},{"cell_type":"code","metadata":{"id":"fpJ8-7h5ns5y","executionInfo":{"status":"ok","timestamp":1617400523093,"user_tz":240,"elapsed":567685,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["def date_generation(date):\n","    # Create date object from given time format in dataframe\n","    my_date = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S+00:00\")\n","    return my_date.date()\n","\n","def date_cleanse(input_df):\n","    input_df['Date'] = input_df['Datetime'].apply(lambda x: date_generation(x))\n","    return input_df\n","\n","# Creating clean date columns\n","df_2017 = date_cleanse(df_2017)\n","df_2018 = date_cleanse(df_2018)\n","df_2019 = date_cleanse(df_2019)\n","df_2020 = date_cleanse(df_2020)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRW6JTK9p8KN"},"source":["## Cleaning Tweet content\n","###### *Removing links, mentions, hashtags etc.*"]},{"cell_type":"code","metadata":{"id":"WlFG1wqfucuv","executionInfo":{"status":"ok","timestamp":1617400523093,"user_tz":240,"elapsed":567681,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["#Generating clean tweets from current text\n","df_2017 = gen_clean_tweets(df_2017, 'Text')\n","df_2018 = gen_clean_tweets(df_2018, 'Text')\n","df_2019 = gen_clean_tweets(df_2019, 'Text')\n","df_2020 = gen_clean_tweets(df_2020, 'Text')"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZyvnUwJTdNO"},"source":["## Tweet Sentiment Calculations\n","###### *Generating the sentiment score for each tweet using the trained LSTM text classifier model*"]},{"cell_type":"code","metadata":{"id":"tr7LImZDfBU1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617400764382,"user_tz":240,"elapsed":808967,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"011634d2-df7d-49aa-fe57-02325f771ee9"},"source":["def get_sentiment(tweet, sent_model): \n","    \n","    #Vectorizing and Padding tweet using max length of Microsoft tweets\n","    tweet_tokenized = tokenizer.texts_to_sequences([tweet])\n","    tweet_padded = pad_sequences(tweet_tokenized, padding='post', maxlen=300)\n","\n","    #Generating prediction using trained model\n","    pred = sent_model.predict(tweet_padded)\n","    \n","    #Extracting the sentiment confidence score\n","    if (np.argmax(pred[0]) == 0):\n","        sentiment_score = -1 * pred[0][0]\n","    elif (np.argmax(pred[0]) == 1):\n","        sentiment_score = pred[0][1] \n","\n","    return sentiment_score\n","\n","#Creates a sentiment score for each tweet using model and microsoft tweets\n","def sentiment_generation(sent_model, input_df): \n","\n","    max_len = input_df['Text'].apply(len).max()\n","    input_df['TwitterSentiment'] = input_df['Text'].apply(get_sentiment, sent_model = sent_model)\n","    sentiment_df = input_df\n","\n","    return sentiment_df \n","\n","df_2017 = sentiment_generation(model, df_2017)\n","df_2018 = sentiment_generation(model, df_2018)\n","df_2019 = sentiment_generation(model, df_2019)\n","df_2020 = sentiment_generation(model, df_2020)\n","\n","print(df_2017[['Text', 'TwitterSentiment']].head(10))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["                                                                                           Text  \\\n","0         Lumia 550 and 650 discounted at online Microsoft Store if you can find them in stock    \n","1  Tesco still has Microsoft Lumia devices in stock as well as its approved partners in the UK    \n","2                                              Why is the Microsoft Store pruning Lumia stock?    \n","3                                              Why is the Microsoft Store pruning Lumia stock?    \n","4                                          6 Signs You Should Sell Microsoft Corporation Stock    \n","5                                  3 Strengths That Make Microsoft Corporation Stock A Buy Now    \n","6                                  3 Strengths That Make Microsoft Corporation Stock A Buy Now    \n","7                                  3 Strengths That Make Microsoft Corporation Stock A Buy Now    \n","8                         Microsoft Corporation Stock Shares Spike Down As microsoftcorporation   \n","9                         Microsoft Corporation Stock Shares Spike Down As microsoftcorporation   \n","\n","   TwitterSentiment  \n","0         -0.818910  \n","1         -0.747336  \n","2         -0.729024  \n","3         -0.729024  \n","4         -0.718287  \n","5         -0.699946  \n","6         -0.699946  \n","7         -0.699946  \n","8         -0.779638  \n","9         -0.779638  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-BIlsbK7qAbt"},"source":["##Sentiment Score Aggregation\n","###### *Aggregating the final sentiment scores to gather scores for each day*"]},{"cell_type":"code","metadata":{"id":"CMsrew4CqJst","executionInfo":{"status":"ok","timestamp":1617400764383,"user_tz":240,"elapsed":808964,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}}},"source":["#Creates an aggregated sentiment score for each day of tweets\n","def aggregated_df (input_df):\n","    agg_df = input_df[['Date', 'TwitterSentiment']]\n","    agg_df =  input_df.groupby([\"Date\"], as_index=False)['TwitterSentiment'].mean()\n","    return agg_df\n","\n","final_2017 = aggregated_df(df_2017)\n","final_2018 = aggregated_df(df_2018)\n","final_2019 = aggregated_df(df_2019)\n","final_2020 = aggregated_df(df_2020)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKzZLKb23bot"},"source":["##Creating full input dataframe for further analysis and stock price predictions\n","###### *Combining twitter data from 2017,2018,2019,2020*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAONYIidb4w2","executionInfo":{"status":"ok","timestamp":1617400764383,"user_tz":240,"elapsed":808961,"user":{"displayName":"Dhanya Dave","photoUrl":"","userId":"06967402246046115679"}},"outputId":"dfc6aab6-a6dd-4119-aea0-a74e8704fb06"},"source":["#Creating full twitter sentiment data\n","twitter_data = final_2017.append([final_2018, final_2019, final_2020])\n","print(twitter_data.shape)\n","print(twitter_data.head())\n","print(twitter_data.tail())\n","\n","twitter_data.to_csv('/content/msft-tweet-sentiments-lstm.csv', index=False, encoding= 'utf-8-sig') "],"execution_count":17,"outputs":[{"output_type":"stream","text":["(1318, 2)\n","         Date  TwitterSentiment\n","0  2017-01-01         -0.818910\n","1  2017-01-02         -0.730918\n","2  2017-01-03         -0.719869\n","3  2017-01-04         -0.771720\n","4  2017-01-05         -0.286388\n","           Date  TwitterSentiment\n","359  2020-12-25          0.090109\n","360  2020-12-26          0.104702\n","361  2020-12-27          0.858708\n","362  2020-12-28          0.464349\n","363  2020-12-29          0.214132\n"],"name":"stdout"}]}]}